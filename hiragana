import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import utils
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers.experimental.preprocessing import Rescaling


batch_size = 256
image_size = (100, 100)

datagen = ImageDataGenerator(rescale = 1./255)

train_dataset = image_dataset_from_directory('/Users/nastya/Documents/2021-2022/diplom/hiragana/images/test resize/5_group_hiragana',
                                             subset='training',
                                             seed=42,
                                             validation_split=0.1,
                                             batch_size=batch_size,
                                             color_mode='grayscale',
                                             image_size=image_size)

validation_dataset = image_dataset_from_directory('/Users/nastya/Documents/2021-2022/diplom/hiragana/images/test resize/5_group_hiragana',
                                            subset='validation',
                                            seed=42,
                                            validation_split=0.1,
                                            batch_size=batch_size,
                                            color_mode='grayscale',
                                            image_size=image_size)

test_dataset = image_dataset_from_directory('/Users/nastya/Documents/2021-2022/diplom/hiragana/images/test resize/5_group_hiragana',
                                            batch_size=batch_size,
                                            color_mode='grayscale',
                                            image_size=image_size)


class_names = train_dataset.class_names
print(class_names)
num = len(class_names)
plt.figure(figsize=(8, 8))

"""for image, labels in train_dataset.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(class_names[labels[i]])
        plt.axis('off')
    #plt.show()"""

AUTOTUNE = tf.data.experimental.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

"""normalization_layer = layers.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))"""


# Создаем последовательную модель
model = Sequential()
# Add convolution 2D
model.add(Rescaling(1./255, input_shape=(100, 100, 1)))
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding="same", kernel_initializer='he_normal', input_shape=(100, 100, 1)))
model.add(Conv2D(32,kernel_size=(5, 5), strides=2, activation='relu'))
model.add(Conv2D(32,kernel_size=(3, 3), strides=2, activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3),  padding='same',activation='relu'))
model.add(Conv2D(64, kernel_size=(5, 5), strides=2, padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3, 3),  padding='same',activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Dropout(0.2))
model.add(Conv2D(128, kernel_size=(5, 5),  padding='same', activation='relu'))
model.add(Conv2D(128, kernel_size=(3, 3),  padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))
# Add dropouts to the model
model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(512, activation='relu'))
model.add(Dense(124, activation='relu'))
# Add dropouts to the model
model.add(Dropout(0.2))
model.add(Dense(num, activation='softmax'))
print(model.summary())

"""model.add(Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', input_shape=(100, 100, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (5, 5), activation='relu', strides=2, padding='same', input_shape=(100, 100, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', strides=2, padding='same', input_shape=(100, 100, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Conv2D(128, (5, 5), activation='relu', strides=2, padding='same', input_shape=(100, 100, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(100, 100, 1)))

model.add(Flatten(input_shape=(100, 100, 1)))

model.add(Dense(372, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(124, activation='relu'))
model.add(Dense(62, activation='softmax'))
"""
"""model = keras.Sequential([
    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(100, 100, 3)),
    MaxPooling2D((2, 2), strides=2),
    Conv2D(64, (3, 3), padding='same', activation='relu'),
    MaxPooling2D((2, 2), strides=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])"""

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

callback = ModelCheckpoint('hiragana_model_resize_for_5_group.m1', #....
              monitor='acc',
              mode='max',
              save_best_only=True)

history = model.fit(train_dataset,
                    validation_data=validation_dataset,
                    epochs=20,
                    verbose=2,
                    callbacks=[callback])

scores = model.evaluate(test_dataset, verbose=1)
model.save('hiragana_model_resize_for_5_group.m1')

plt.plot(history.history['accuracy'],
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'],
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()
